


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>xFormers optimized operators | xFormers 0.0.22 documentation</title>
  
  <script src="../_static/js/ga.js"></script>
  <script src="../_static/js/redirect.js"></script>
  
  <link rel="shortcut icon" href="../_static/images/favicon.png" />
  
  
  <link rel="canonical" href="https://facebookresearch.github.io/xformerscomponents/ops.html" />
  
  <meta property="og:title" content="xFormers optimized operators | xFormers 0.0.22 documentation">
  <meta name="description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <meta property="og:description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <!--<meta property="og:image" content="https://mmf.sh/img/logo.png">-->
  <!--<meta property="twitter:image" content="https://mmf.sh/img/logo.png">-->
  <meta name="twitter:image:alt" content="Image for xFormers">
  <meta name="twitter:card" content="summary_large_image">
  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/css/customize.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="Attention mechanisms" href="attentions.html" />
  <link rel="prev" title="API Reference" href="index.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/facebookresearch/xformers"><img
          src="../_static/logo.png" style="padding-right: 90px;" width="280px" height="53px"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers"> xFormers Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../what_is_xformers.html">What is xFormers?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Components Documentation</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build models and blocks programatically</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../factory/index.html">Factory</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials and examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Some custom parts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../custom_parts/index.html">Custom parts reference</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">API Reference</a> &gt;</li>
        
      <li>xFormers optimized operators</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/components/ops.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="xformers-optimized-operators">
<h1>xFormers optimized operators<a class="headerlink" href="#xformers-optimized-operators" title="Permalink to this heading">¶</a></h1>
<section id="module-xformers.ops">
<span id="memory-efficient-attention"></span><h2>Memory-efficient attention<a class="headerlink" href="#module-xformers.ops" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="xformers.ops.memory_efficient_attention">
<span class="sig-prename descclassname"><span class="pre">xformers.ops.</span></span><span class="sig-name descname"><span class="pre">memory_efficient_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Union" title="(in Python v3.6)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#xformers.ops.fmha.attn_bias.AttentionBias" title="xformers.ops.fmha.attn_bias.AttentionBias"><span class="pre">AttentionBias</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><span class="pre">float</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Tuple" title="(in Python v3.6)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Type" title="(in Python v3.6)"><span class="pre">Type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">AttentionFwOpBase</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Type" title="(in Python v3.6)"><span class="pre">Type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">AttentionBwOpBase</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha.html#memory_efficient_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.memory_efficient_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the memory-efficient attention mechanism following
<a class="reference external" href="http://arxiv.org/abs/2112.05682">“Self-Attention Does Not Need O(n^2) Memory”</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Inputs shape</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<ul class="simple">
<li><p>Input tensors must be in format <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">M,</span> <span class="pre">H,</span> <span class="pre">K]</span></code>, where B is the batch size, M         the sequence length, H the number of heads, and K the embeding size per head</p></li>
<li><p>If inputs have dimension 3, it is assumed that the dimensions are <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">M,</span> <span class="pre">K]</span></code> and <code class="docutils literal notranslate"><span class="pre">H=1</span></code></p></li>
<li><p>Inputs can be non-contiguous - we only require the last dimension’s stride to be 1</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Equivalent pytorch code</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">query</span> <span class="o">*</span> <span class="n">scale</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">query</span> <span class="o">@</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">if</span> <span class="n">attn_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">+</span> <span class="n">attn_bias</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="k">return</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">value</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Examples</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">xformers.ops</span> <span class="k">as</span> <span class="nn">xops</span>

<span class="c1"># Compute regular attention</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

<span class="c1"># With a dropout of 0.2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Causal attention</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention</span><span class="p">(</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span>
    <span class="n">attn_bias</span><span class="o">=</span><span class="n">xops</span><span class="o">.</span><span class="n">LowerTriangularMask</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Supported hardware</dt>
<dd class="field-odd"><p>NVIDIA GPUs with compute capability above 6.0 (P100+), datatype <code class="docutils literal notranslate"><span class="pre">f16</span></code>, <code class="docutils literal notranslate"><span class="pre">bf16</span></code> and <code class="docutils literal notranslate"><span class="pre">f32</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#NotImplementedError" title="(in Python v3.6)"><strong>NotImplementedError</strong></a> – if there is no operator available to compute the MHA</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if inputs are invalid</p></li>
</ul>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> – Tensor of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">Mq,</span> <span class="pre">H,</span> <span class="pre">K]</span></code></p></li>
<li><p><strong>key</strong> – Tensor of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">Mkv,</span> <span class="pre">H,</span> <span class="pre">K]</span></code></p></li>
<li><p><strong>value</strong> – Tensor of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">Mkv,</span> <span class="pre">H,</span> <span class="pre">Kv]</span></code></p></li>
<li><p><strong>attn_bias</strong> – Bias to apply to the attention matrix - defaults to no masking.         For common biases implemented efficiently in xFormers, see <a class="reference internal" href="#xformers.ops.fmha.attn_bias.AttentionBias" title="xformers.ops.fmha.attn_bias.AttentionBias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.AttentionBias</span></code></a>.         This can also be a <code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.Tensor</span></code> for an arbitrary mask (slower).</p></li>
<li><p><strong>p</strong> – Dropout probability. Disabled if set to <code class="docutils literal notranslate"><span class="pre">0.0</span></code></p></li>
<li><p><strong>scale</strong> – Scaling factor for <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K.transpose()</span></code>. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, the default         scale (q.shape[-1]**-0.5) will be used.</p></li>
<li><p><strong>op</strong> – The operators to use - see <a class="reference internal" href="#xformers.ops.AttentionOpBase" title="xformers.ops.AttentionOpBase"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.AttentionOpBase</span></code></a>.         If set to <code class="docutils literal notranslate"><span class="pre">None</span></code> (recommended), xFormers         will dispatch to the best available operator, depending on the inputs         and options.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>multi-head attention Tensor with shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">Mq,</span> <span class="pre">H,</span> <span class="pre">Kv]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.AttentionBias">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.</span></span><span class="sig-name descname"><span class="pre">AttentionBias</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#AttentionBias"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.AttentionBias" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<dl class="simple">
<dt>Base class for a custom bias that can be applied         as the attn_bias argument in</dt><dd><p><a class="reference internal" href="#xformers.ops.memory_efficient_attention" title="xformers.ops.memory_efficient_attention"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.memory_efficient_attention</span></code></a>.</p>
</dd>
</dl>
<p>That function has the ability to add a tensor, the
attention bias, to the QK^T matrix before it is used
in the softmax part of the attention calculation.
The attention bias tensor with shape
(B or 1, n_queries, number of keys)
can be given as the attn_bias input.
The most common use case is for an attention bias is
to contain only zeros and negative infinities, which forms
a mask so that some queries only attend to some keys.</p>
<p>Children of this class define alternative things which can
be used as the attn_bias input to define an attention bias which
forms such a mask, for some common cases.</p>
<p>When using an <a class="reference internal" href="#xformers.ops.AttentionBias" title="xformers.ops.AttentionBias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.AttentionBias</span></code></a>
instead of a <code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, the mask matrix does
not need to be materialized, and can be
hardcoded into some kernels for better performance.</p>
<p>See:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#xformers.ops.fmha.attn_bias.LowerTriangularMask" title="xformers.ops.fmha.attn_bias.LowerTriangularMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.LowerTriangularMask</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias" title="xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.BlockDiagonalMask</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask</span></code></a></p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.AttentionBias.materialize">
<span class="sig-name descname"><span class="pre">materialize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Tuple" title="(in Python v3.6)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensor_attributes.html#torch.dtype" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">dtype</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Union" title="(in Python v3.6)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://pytorch.org/docs/master/tensor_attributes.html#torch.device" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">device</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#AttentionBias.materialize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.AttentionBias.materialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Materializes the bias as a <cite>torch.Tensor</cite>. This is very slow
and we don’t attempt to make it fast. Only use for debugging/testing.</p>
<p>Shape should be like <cite>[*, q_seqlen, k_seqlen]</cite></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.AttentionOpBase">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.</span></span><span class="sig-name descname"><span class="pre">AttentionOpBase</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/common.html#AttentionOpBase"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.AttentionOpBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseOperator</span></code></p>
<p>Base class for any attention operator in xFormers</p>
<p>See:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#xformers.ops.fmha.cutlass.FwOp" title="xformers.ops.fmha.cutlass.FwOp"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.cutlass.FwOp</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.cutlass.BwOp" title="xformers.ops.fmha.cutlass.BwOp"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.cutlass.BwOp</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.flash.FwOp" title="xformers.ops.fmha.flash.FwOp"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.flash.FwOp</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.flash.BwOp" title="xformers.ops.fmha.flash.BwOp"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.flash.BwOp</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.triton.FwOp" title="xformers.ops.fmha.triton.FwOp"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.triton.FwOp</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.triton.BwOp" title="xformers.ops.fmha.triton.BwOp"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.triton.BwOp</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.small_k.FwOp" title="xformers.ops.fmha.small_k.FwOp"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.small_k.FwOp</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.small_k.BwOp" title="xformers.ops.fmha.small_k.BwOp"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.small_k.BwOp</span></code></a></p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.AttentionOpBase.not_supported_reasons">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">not_supported_reasons</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Inputs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.List" title="(in Python v3.6)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/common.html#AttentionOpBase.not_supported_reasons"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.AttentionOpBase.not_supported_reasons" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of reasons why this is not supported.
The kernel can run these inputs only if the returned list is empty</p>
</dd></dl>

</dd></dl>

<section id="module-xformers.ops.fmha.cutlass">
<span id="available-implementations"></span><h3>Available implementations<a class="headerlink" href="#module-xformers.ops.fmha.cutlass" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.cutlass.FwOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.cutlass.</span></span><span class="sig-name descname"><span class="pre">FwOp</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/cutlass.html#FwOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.cutlass.FwOp" title="Permalink to this definition">¶</a></dt>
<dd><p>xFormers’ MHA kernel based on CUTLASS.
Supports a large number of settings (including without TensorCores, f32 …)
and GPUs as old as P100 (Sm60)</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.cutlass.BwOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.cutlass.</span></span><span class="sig-name descname"><span class="pre">BwOp</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/cutlass.html#BwOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.cutlass.BwOp" title="Permalink to this definition">¶</a></dt>
<dd><p>xFormers’ MHA kernel based on CUTLASS.
Supports a large number of settings (including without TensorCores, f32 …)
and GPUs as old as P100 (Sm60)</p>
</dd></dl>

<span class="target" id="module-xformers.ops.fmha.flash"></span><dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.flash.FwOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.flash.</span></span><span class="sig-name descname"><span class="pre">FwOp</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/flash.html#FwOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.flash.FwOp" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator that computes memory-efficient attention using         <a class="reference external" href="https://github.com/HazyResearch/flash-attention">Flash-Attention</a>         implementation.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.flash.BwOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.flash.</span></span><span class="sig-name descname"><span class="pre">BwOp</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/flash.html#BwOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.flash.BwOp" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator that computes memory-efficient attention using         <a class="reference external" href="https://github.com/HazyResearch/flash-attention">Flash-Attention</a>         implementation.</p>
</dd></dl>

<span class="target" id="module-xformers.ops.fmha.triton"></span><dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.triton.FwOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.triton.</span></span><span class="sig-name descname"><span class="pre">FwOp</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/triton.html#FwOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.triton.FwOp" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator that computes memory-efficient attention using         <a class="reference external" href="https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attn_triton.py">Tri Dao’s</a>         implementation, based on
<a class="reference external" href="https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py">Phil Tillet’s code</a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.triton.BwOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.triton.</span></span><span class="sig-name descname"><span class="pre">BwOp</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/triton.html#BwOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.triton.BwOp" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator that computes memory-efficient attention using         <a class="reference external" href="https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attn_triton.py">Tri Dao’s</a>         implementation, based on
<a class="reference external" href="https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py">Phil Tillet’s code</a></p>
</dd></dl>

<span class="target" id="module-xformers.ops.fmha.small_k"></span><dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.small_k.FwOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.small_k.</span></span><span class="sig-name descname"><span class="pre">FwOp</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/small_k.html#FwOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.small_k.FwOp" title="Permalink to this definition">¶</a></dt>
<dd><p>An operator optimized for very small values of K (<code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">&lt;=</span> <span class="pre">32</span></code>)         and f32 pre-Ampere as it does not use TensorCores.
Only supports contiguous inputs in BMK format, so an extra reshape         or contiguous call might be done.</p>
<dl class="field-list simple">
<dt class="field-odd">Deprecated</dt>
<dd class="field-odd"><p>This operator is deprecated and should not be used in new code</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.small_k.BwOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.small_k.</span></span><span class="sig-name descname"><span class="pre">BwOp</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/small_k.html#BwOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.small_k.BwOp" title="Permalink to this definition">¶</a></dt>
<dd><p>An operator optimized for very small values of K (<code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">&lt;=</span> <span class="pre">32</span></code>)         and f32 pre-Ampere as it does not use TensorCores.
Only supports contiguous inputs in BMK format, so an extra reshape         or contiguous call might be done.</p>
<dl class="field-list simple">
<dt class="field-odd">Deprecated</dt>
<dd class="field-odd"><p>This operator is deprecated and should not be used in new code</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-xformers.ops.fmha.attn_bias">
<span id="attention-biases"></span><h3>Attention biases<a class="headerlink" href="#module-xformers.ops.fmha.attn_bias" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.AttentionBias">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.attn_bias.</span></span><span class="sig-name descname"><span class="pre">AttentionBias</span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#AttentionBias"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.AttentionBias" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<dl class="simple">
<dt>Base class for a custom bias that can be applied         as the attn_bias argument in</dt><dd><p><a class="reference internal" href="#xformers.ops.memory_efficient_attention" title="xformers.ops.memory_efficient_attention"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.memory_efficient_attention</span></code></a>.</p>
</dd>
</dl>
<p>That function has the ability to add a tensor, the
attention bias, to the QK^T matrix before it is used
in the softmax part of the attention calculation.
The attention bias tensor with shape
(B or 1, n_queries, number of keys)
can be given as the attn_bias input.
The most common use case is for an attention bias is
to contain only zeros and negative infinities, which forms
a mask so that some queries only attend to some keys.</p>
<p>Children of this class define alternative things which can
be used as the attn_bias input to define an attention bias which
forms such a mask, for some common cases.</p>
<p>When using an <a class="reference internal" href="#xformers.ops.AttentionBias" title="xformers.ops.AttentionBias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.AttentionBias</span></code></a>
instead of a <code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, the mask matrix does
not need to be materialized, and can be
hardcoded into some kernels for better performance.</p>
<p>See:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#xformers.ops.fmha.attn_bias.LowerTriangularMask" title="xformers.ops.fmha.attn_bias.LowerTriangularMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.LowerTriangularMask</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias" title="xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.BlockDiagonalMask</span></code></a></p></li>
<li><p><a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask</span></code></a></p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.AttentionBias.materialize">
<span class="sig-name descname"><span class="pre">materialize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Tuple" title="(in Python v3.6)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensor_attributes.html#torch.dtype" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">dtype</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Union" title="(in Python v3.6)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://pytorch.org/docs/master/tensor_attributes.html#torch.device" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">device</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#AttentionBias.materialize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.AttentionBias.materialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Materializes the bias as a <cite>torch.Tensor</cite>. This is very slow
and we don’t attempt to make it fast. Only use for debugging/testing.</p>
<p>Shape should be like <cite>[*, q_seqlen, k_seqlen]</cite></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.LowerTriangularMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.attn_bias.</span></span><span class="sig-name descname"><span class="pre">LowerTriangularMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensor_args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">tensor_kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#LowerTriangularMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.LowerTriangularMask" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#xformers.ops.fmha.attn_bias.AttentionBias" title="xformers.ops.fmha.attn_bias.AttentionBias"><code class="xref py py-class docutils literal notranslate"><span class="pre">AttentionBias</span></code></a></p>
<p>A lower-triangular (aka causal) mask</p>
<p>A query Q cannot attend to a key which is farther from the
initial key than Q is from the initial query.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.attn_bias.</span></span><span class="sig-name descname"><span class="pre">LowerTriangularMaskWithTensorBias</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#LowerTriangularMaskWithTensorBias"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#xformers.ops.fmha.attn_bias.LowerTriangularMask" title="xformers.ops.fmha.attn_bias.LowerTriangularMask"><code class="xref py py-class docutils literal notranslate"><span class="pre">LowerTriangularMask</span></code></a></p>
<p>A lower-triangular (aka causal) mask with an additive bias</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.attn_bias.</span></span><span class="sig-name descname"><span class="pre">BlockDiagonalMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_seqinfo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_SeqLenInfo</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k_seqinfo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_SeqLenInfo</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_batch_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Sequence" title="(in Python v3.6)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#xformers.ops.fmha.attn_bias.AttentionBias" title="xformers.ops.fmha.attn_bias.AttentionBias"><code class="xref py py-class docutils literal notranslate"><span class="pre">AttentionBias</span></code></a></p>
<p>A block-diagonal mask that can be passed as <code class="docutils literal notranslate"><span class="pre">attn_bias</span></code>
argument to <a class="reference internal" href="#xformers.ops.memory_efficient_attention" title="xformers.ops.memory_efficient_attention"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.memory_efficient_attention</span></code></a>.</p>
<p>Queries and Keys are each divided into the same number of blocks.
Queries in block i only attend to keys in block i.</p>
<figure class="align-default" id="id4">
<img alt="../_images/block_diag_bias.png" src="../_images/block_diag_bias.png" />
<figcaption>
<p><span class="caption-text">This bias can be used to handle a batch of sequences of
different lengths, via <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask.from_tensor_list" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask.from_tensor_list"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BlockDiagonalMask.from_tensor_list</span></code></a></span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<dl class="field-list simple">
<dt class="field-odd">Example</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">xformers.ops</span> <span class="kn">import</span> <span class="n">fmha</span>

<span class="n">K</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
<span class="n">list_x</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
<span class="p">]</span>
<span class="n">attn_bias</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">fmha</span><span class="o">.</span><span class="n">BlockDiagonalMask</span><span class="o">.</span><span class="n">from_tensor_list</span><span class="p">(</span><span class="n">list_x</span><span class="p">)</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">K</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">K</span><span class="p">])</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">fmha</span><span class="o">.</span><span class="n">memory_efficient_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">)</span>
<span class="n">list_out</span> <span class="o">=</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">list_out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># [1, 3, 1, K]</span>
<span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">list_out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalMask.materialize">
<span class="sig-name descname"><span class="pre">materialize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Tuple" title="(in Python v3.6)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensor_attributes.html#torch.dtype" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">dtype</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Union" title="(in Python v3.6)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://pytorch.org/docs/master/tensor_attributes.html#torch.device" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">device</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask.materialize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask.materialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Materialize the attention bias - for debugging &amp; testing</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalMask.from_seqlens">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_seqlens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_seqlen</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Sequence" title="(in Python v3.6)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_seqlen</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Sequence" title="(in Python v3.6)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><span class="pre">BlockDiagonalMask</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask.from_seqlens"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask.from_seqlens" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BlockDiagonalMask</span></code></a> from a list of tensors lengths for query and key/value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>q_seqlen</strong> (<em>Union</em><em>[</em><em>Sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em><em>, </em><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><em>torch.Tensor</em></a><em>]</em>) – List or tensor of sequence lengths for query tensors</p></li>
<li><p><strong>kv_seqlen</strong> (<em>Union</em><em>[</em><em>Sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em><em>, </em><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><em>torch.Tensor</em></a><em>]</em><em>, </em><em>optional</em>) – List or tensor of sequence lengths for key/value.
(Defaults to <code class="docutils literal notranslate"><span class="pre">q_seqlen</span></code>.)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>BlockDiagonalMask</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalMask.from_tensor_list">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_tensor_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Sequence" title="(in Python v3.6)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Tuple" title="(in Python v3.6)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><span class="pre">BlockDiagonalMask</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask.from_tensor_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask.from_tensor_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BlockDiagonalMask</span></code></a> from a list of tensors, and returns the tensors
concatenated on the sequence length dimension</p>
<figure class="align-default" id="id5">
<img alt="../_images/block_diag_cat_split.png" src="../_images/block_diag_cat_split.png" />
<figcaption>
<p><span class="caption-text">See also <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask.split" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask.split"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BlockDiagonalMask.split</span></code></a> to split the returned
<code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.Tensor</span></code> back to a list of tensors of varying sequence length</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensors</strong> (<em>Sequence</em><em>[</em><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><em>torch.Tensor</em></a><em>]</em>) – A list of tensors of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">M_i,</span> <span class="pre">*]</span></code>.
All tensors should have the same dimension and the same batch size <code class="docutils literal notranslate"><span class="pre">B</span></code>, but
they can have different sequence length <code class="docutils literal notranslate"><span class="pre">M</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>Tuple[BlockDiagonalMask, torch.Tensor]</em> – The corresponding bias for the attention
along with <cite>tensors</cite> concatenated on the sequence length dimension, with shape <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">sum_i{M_i},</span> <span class="pre">*]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalMask.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Sequence" title="(in Python v3.6)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask.split"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask.split" title="Permalink to this definition">¶</a></dt>
<dd><p>The inverse operation of <code class="xref py py-attr docutils literal notranslate"><span class="pre">BlockDiagonalCausalMask.from_tensor_list</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><em>torch.Tensor</em></a>) – Tensor of tokens of shape <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">sum_i{M_i},</span> <span class="pre">*]</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>Sequence[torch.Tensor]</em> – A list of tokens with possibly different sequence lengths</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalMask.make_causal">
<span class="sig-name descname"><span class="pre">make_causal</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask"><span class="pre">BlockDiagonalCausalMask</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask.make_causal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask.make_causal" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes each block causal</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalMask.make_causal_from_bottomright">
<span class="sig-name descname"><span class="pre">make_causal_from_bottomright</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalFromBottomRightMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalCausalFromBottomRightMask"><span class="pre">BlockDiagonalCausalFromBottomRightMask</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask.make_causal_from_bottomright"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask.make_causal_from_bottomright" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes each block causal with a possible non-causal prefix</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.attn_bias.</span></span><span class="sig-name descname"><span class="pre">BlockDiagonalCausalMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_seqinfo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_SeqLenInfo</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k_seqinfo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_SeqLenInfo</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_batch_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Sequence" title="(in Python v3.6)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalCausalMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlockDiagonalMask</span></code></a></p>
<p>Same as <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.BlockDiagonalMask</span></code></a>, except that each block is causal.</p>
<p>Queries and Keys are each divided into the same number of blocks.
A query Q in block i cannot attend to a key which is not in block i,
nor one which is farther from the initial key in block i than Q
is from the initial query in block i.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalCausalFromBottomRightMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.attn_bias.</span></span><span class="sig-name descname"><span class="pre">BlockDiagonalCausalFromBottomRightMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_seqinfo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_SeqLenInfo</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k_seqinfo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_SeqLenInfo</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_batch_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Sequence" title="(in Python v3.6)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalCausalFromBottomRightMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalFromBottomRightMask" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlockDiagonalMask</span></code></a></p>
<p>Same as <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.BlockDiagonalMask</span></code></a>, except that each block is causal.
This mask allows for a non-causal prefix
NOTE: Each block should have <cite>num_keys &gt;= num_queries</cite> otherwise the forward pass is not
defined (softmax of vector of <cite>-inf</cite> in the attention)</p>
<p>Queries and keys are each divided into the same number of blocks.
A query Q in block i cannot attend to a key which is not in block i,
nor one which nearer the final key in block i than Q is to the
final query in block i.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.attn_bias.</span></span><span class="sig-name descname"><span class="pre">BlockDiagonalCausalWithOffsetPaddedKeysMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_seqinfo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_SeqLenInfo</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k_seqinfo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_PaddedSeqLenInfo</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal_diagonal</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Any" title="(in Python v3.6)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalCausalWithOffsetPaddedKeysMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#xformers.ops.fmha.attn_bias.AttentionBias" title="xformers.ops.fmha.attn_bias.AttentionBias"><code class="xref py py-class docutils literal notranslate"><span class="pre">AttentionBias</span></code></a></p>
<p>Same as <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask</span></code></a>,
except an offset on causality is allowed for each block and we support padding for k/v</p>
<p>The keys and values are divided into blocks which are padded out to
the same total length.
For example, if there is space for 12 keys, for three blocks of
max length 4, but we only want to use the first 2, 3 and 2
of each block, use <cite>kv_padding=4</cite> and <cite>kv_seqlens=[2, 3, 2]</cite>.
The queries are divided into blocks, without padding, of lengths given by
q_seqlen.</p>
<p>A query Q in block i cannot attend to a key which is not in block i,
nor one which is not in use (i.e. in the padded area),
nor one which is nearer to the final key in block i
than Q is to the final query in block i.</p>
<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask.materialize">
<span class="sig-name descname"><span class="pre">materialize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Tuple" title="(in Python v3.6)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensor_attributes.html#torch.dtype" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">dtype</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Union" title="(in Python v3.6)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://pytorch.org/docs/master/tensor_attributes.html#torch.device" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">device</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalCausalWithOffsetPaddedKeysMask.materialize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask.materialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Materialize the attention bias - for debugging &amp; testing</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_seqlens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_seqlen</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Sequence" title="(in Python v3.6)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_seqlen</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Sequence" title="(in Python v3.6)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal_diagonal</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Any" title="(in Python v3.6)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask"><span class="pre">BlockDiagonalCausalWithOffsetPaddedKeysMask</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask" title="xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BlockDiagonalCausalWithOffsetPaddedKeysMask</span></code></a> from a list of tensor
lengths for query and key/value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>q_seqlen</strong> (<em>Sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – List or tensor of sequence lengths for query tensors</p></li>
<li><p><strong>kv_padding</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Padding for k/v - also an upperbound on each individual key length</p></li>
<li><p><strong>kv_seqlen</strong> (<em>Sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – List or tensor of sequence lengths for key/value.</p></li>
<li><p><strong>causal_diagonal</strong> – unused, for BC only</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>BlockDiagonalCausalWithOffsetPaddedKeysMask</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-xformers.ops.fmha">
<span id="non-autograd-implementations"></span><h3>Non-autograd implementations<a class="headerlink" href="#module-xformers.ops.fmha" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="xformers.ops.fmha.memory_efficient_attention_backward">
<span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.</span></span><span class="sig-name descname"><span class="pre">memory_efficient_attention_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">lse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Union" title="(in Python v3.6)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#xformers.ops.fmha.attn_bias.AttentionBias" title="xformers.ops.fmha.attn_bias.AttentionBias"><span class="pre">AttentionBias</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><span class="pre">float</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Type" title="(in Python v3.6)"><span class="pre">Type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">AttentionBwOpBase</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Tuple" title="(in Python v3.6)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha.html#memory_efficient_attention_backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.memory_efficient_attention_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the gradient of the attention.
Returns a tuple (dq, dk, dv)
See <a class="reference internal" href="#xformers.ops.memory_efficient_attention" title="xformers.ops.memory_efficient_attention"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.memory_efficient_attention</span></code></a> for an explanation of the arguments.
<cite>lse</cite> is the tensor returned by <code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.memory_efficient_attention_forward_requires_grad</span></code></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="xformers.ops.fmha.memory_efficient_attention_forward">
<span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.</span></span><span class="sig-name descname"><span class="pre">memory_efficient_attention_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Union" title="(in Python v3.6)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#xformers.ops.fmha.attn_bias.AttentionBias" title="xformers.ops.fmha.attn_bias.AttentionBias"><span class="pre">AttentionBias</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><span class="pre">float</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Type" title="(in Python v3.6)"><span class="pre">Type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">AttentionFwOpBase</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha.html#memory_efficient_attention_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.memory_efficient_attention_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the forward pass of <a class="reference internal" href="#xformers.ops.memory_efficient_attention" title="xformers.ops.memory_efficient_attention"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.memory_efficient_attention</span></code></a>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="xformers.ops.fmha.memory_efficient_attention_forward_requires_grad">
<span class="sig-prename descclassname"><span class="pre">xformers.ops.fmha.</span></span><span class="sig-name descname"><span class="pre">memory_efficient_attention_forward_requires_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Union" title="(in Python v3.6)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#xformers.ops.fmha.attn_bias.AttentionBias" title="xformers.ops.fmha.attn_bias.AttentionBias"><span class="pre">AttentionBias</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><span class="pre">float</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Optional" title="(in Python v3.6)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Type" title="(in Python v3.6)"><span class="pre">Type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">AttentionFwOpBase</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3.6/library/typing.html#typing.Tuple" title="(in Python v3.6)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/xformers/ops/fmha.html#memory_efficient_attention_forward_requires_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#xformers.ops.fmha.memory_efficient_attention_forward_requires_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tuple (output, lse), where <cite>lse</cite> can be used to compute the backward pass later.
See <a class="reference internal" href="#xformers.ops.memory_efficient_attention" title="xformers.ops.memory_efficient_attention"><code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.memory_efficient_attention</span></code></a> for an explanation of the arguments
See <code class="xref py py-attr docutils literal notranslate"><span class="pre">xformers.ops.memory_efficient_attention_backward</span></code> for running the backward pass</p>
</dd></dl>

</section>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="attentions.html" class="btn btn-neutral float-right" title="Attention mechanisms" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="API Reference" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright © 2021 Meta Platforms, Inc.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <!-- <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">xFormers optimized operators</a><ul>
<li><a class="reference internal" href="#module-xformers.ops">Memory-efficient attention</a><ul>
<li><a class="reference internal" href="#module-xformers.ops.fmha.cutlass">Available implementations</a></li>
<li><a class="reference internal" href="#module-xformers.ops.fmha.attn_bias">Attention biases</a></li>
<li><a class="reference internal" href="#module-xformers.ops.fmha">Non-autograd implementations</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div> -->
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/facebookresearch/xformers" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/facebookresearch/xformers">fairscale</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Resources</a></li>
            <li><a href="https://github.com/facebookresearch/xformers">Docs</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://opensource.facebook.com/legal/terms">Terms of Use</a></li>
            <li><a href="https://opensource.facebook.com/legal/privacy">Privacy Policy</a></li>
          </ul>
        </div>
      </div>
    </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/facebookresearch/xformers" aria-label="fairscale"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Docs</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>